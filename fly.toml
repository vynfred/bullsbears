# BullsBears AI Inference Server - Fly.io Configuration
app = "bullsbears-ai"
primary_region = "ord"

[build]
  dockerfile = "Dockerfile.ollama"

[env]
  OLLAMA_HOST = "0.0.0.0:11434"
  OLLAMA_MODELS = "/app/models"

[http_service]
  internal_port = 11434
  force_https = true
  auto_stop_machines = false
  auto_start_machines = true
  min_machines_running = 1
  processes = ["app"]

  [[http_service.checks]]
    grace_period = "10s"
    interval = "30s"
    method = "GET"
    timeout = "5s"
    path = "/api/tags"

[vm]
  cpu_kind = "performance"
  cpus = 8
  memory = "32gb"
  # GPU disabled for now - need approval from Fly.io
  # gpu_kind = "a100-pcie-40gb"
  # gpus = 1

[[mounts]]
  source = "ollama_models"
  destination = "/app/models"
  initial_size = "100gb"

[processes]
  app = "ollama serve"

# Restart policy handled by processes section
