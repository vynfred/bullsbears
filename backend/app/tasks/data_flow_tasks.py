"""
Data Flow Celery Tasks - Following PROJECT_ROADMAP.md Processing Flow
Connects DataFlowManager business logic to Celery scheduling
"""

import asyncio
import logging
from celery import Celery
from ..core.celery_app import celery_app
from ..services.data_flow_manager import DataFlowManager
from ..services.kill_switch_service import KillSwitchService
from ..services.fmp_data_ingestion import get_fmp_service
from ..services.chart_generator import get_chart_generator
from ..services.firebase_service import FirebaseService

logger = logging.getLogger(__name__)

@celery_app.task(name='weekly_data_update')
def weekly_data_update():
    """
    WEEKLY OPERATIONS (Saturday mornings)
    - FMP update ALL tier (fresh NASDAQ universe)
    - Prefilter ALL â†’ ACTIVE (volatility/movement criteria)
    """
    logger.info("ðŸ”„ Starting weekly data update task")
    
    try:
        result = asyncio.run(_run_weekly_update())
        logger.info(f"âœ… Weekly update completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Weekly update failed: {e}")
        raise

@celery_app.task(name='daily_data_update')
def daily_data_update():
    """
    DAILY OPERATIONS (Weekday mornings before market open)
    Phase 1: Tiered Stock Classification
    - FMP update ACTIVE tier only (price/volume updates)
    - ACTIVE weekly review â†’ QUALIFIED tier
    """
    logger.info("ðŸ“Š Starting daily data update task")
    
    try:
        result = asyncio.run(_run_daily_update())
        logger.info(f"âœ… Daily update completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Daily update failed: {e}")
        raise

@celery_app.task(name='daily_prescreen_pipeline')
def daily_prescreen_pipeline():
    """
    CORRECTED: Prescreen Agent Pipeline
    - ACTIVE â†’ QUALIFIED (finma-7b technical/fundamental/sentiment analysis)
    - QUALIFIED â†’ SHORT_LIST (finma-7b final selection for other agents)
    """
    logger.info("ðŸ¤– Starting daily prescreen pipeline (ACTIVE â†’ QUALIFIED â†’ SHORT_LIST)")
    
    try:
        result = asyncio.run(_run_prescreen_pipeline())
        logger.info(f"âœ… Prescreen pipeline completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Prescreen pipeline failed: {e}")
        raise

@celery_app.task(name='run_agent_pipeline')
def run_agent_pipeline():
    """
    Phase 3-8: 16+2 Agent Pipeline on SHORT_LIST
    - Phase 0: Kill Switch Check (VIX >35 + SPY <-2%)
    - Phase 3: Prediction Consensus (8 Local Agents)
    - Phase 4: Vision Analysis (2 Local Agents) 
    - Phase 5: Risk Analysis (2 Local Agents)
    - Phase 6: Target Consensus (2 Local Agents)
    - Phase 7: News & Social Integration (2 Agents)
    - Phase 8: Final Arbitration (DeepSeek-V3 Cloud)
    """
    logger.info("ðŸš€ Starting 16+2 agent pipeline")
    
    try:
        result = asyncio.run(_run_agent_pipeline())
        logger.info(f"âœ… Agent pipeline completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Agent pipeline failed: {e}")
        raise

# ==================== ASYNC EXECUTION FUNCTIONS ====================

async def _run_weekly_update():
    """Execute weekly data flow operations"""
    manager = DataFlowManager()
    await manager.weekly_data_update()
    return {"status": "success", "operation": "weekly_update"}

async def _run_daily_update():
    """Execute daily data flow operations"""
    manager = DataFlowManager()
    await manager.daily_data_update()
    return {"status": "success", "operation": "daily_update"}

async def _run_prescreen_pipeline():
    """Execute prescreen pipeline (ACTIVE â†’ QUALIFIED â†’ SHORT_LIST)"""
    manager = DataFlowManager()
    
    # Prescreen agent makes all tier decisions
    short_list = await manager.run_daily_prescreen_full_pipeline()
    
    return {
        "status": "success", 
        "operation": "prescreen_pipeline",
        "short_list_count": len(short_list)
    }

async def _run_agent_pipeline():
    """Execute 16+2 agent pipeline with kill switch"""
    manager = DataFlowManager()
    kill_switch = KillSwitchService()
    
    # Phase 0: Kill Switch Check
    kill_switch_active = await kill_switch.check_market_conditions()
    if kill_switch_active:
        logger.warning("ðŸ›‘ Kill switch activated - no picks today due to market conditions")
        return {
            "status": "kill_switch_active",
            "operation": "agent_pipeline",
            "picks": [],
            "reason": "Market conditions: VIX >35 or SPY <-2%"
        }
    
    # Phase 3-8: Run full agent pipeline
    picks = await manager.run_agent_pipeline_on_shortlist()
    
    return {
        "status": "success",
        "operation": "agent_pipeline", 
        "picks_generated": len(picks),
        "kill_switch_checked": True
    }

# ==================== ASYNC EXECUTION FUNCTIONS FOR NEW TASKS ====================

async def _run_fmp_delta_update():
    """Execute FMP daily delta update"""
    async with get_fmp_service() as fmp:
        # Get ACTIVE tier symbols
        manager = DataFlowManager()
        await manager.initialize()

        # Use DataFlowManager's daily update method
        result = await manager.daily_data_update()
        return result

async def _run_build_active_tickers():
    """Execute ACTIVE ticker classification"""
    manager = DataFlowManager()
    await manager.initialize()

    # This is part of the daily update process
    # For now, return success - this logic is in DataFlowManager
    return {"status": "success", "operation": "build_active_tickers"}

async def _run_finma_prescreen():
    """Execute FinMA-7b prescreen via DataFlowManager"""
    manager = DataFlowManager()
    await manager.initialize()

    short_list = await manager.run_daily_prescreen_full_pipeline()

    return {
        "status": "success",
        "operation": "finma_prescreen",
        "short_list_count": len(short_list)
    }

async def _run_generate_charts():
    """Execute chart generation for SHORT_LIST"""
    # Get latest SHORT_LIST from database
    from ..core.database import get_database

    db_pool = await get_database()

    # Get today's SHORT_LIST candidates
    query = """
    SELECT DISTINCT symbol
    FROM pick_candidates
    WHERE analysis_date = CURRENT_DATE
    ORDER BY symbol
    """

    async with db_pool.acquire() as conn:
        rows = await conn.fetch(query)

    symbols = [row['symbol'] for row in rows]

    if not symbols:
        logger.warning("No SHORT_LIST candidates found for chart generation")
        return {"status": "no_candidates", "charts_generated": 0}

    # Generate charts
    async with get_chart_generator() as chart_gen:
        shortlist_data = [{"symbol": symbol} for symbol in symbols]
        charts = await chart_gen.generate_batch(shortlist_data)

    return {
        "status": "success",
        "operation": "generate_charts",
        "charts_generated": len(charts)
    }

async def _run_groq_vision():
    """Execute Groq vision analysis"""
    # This would be handled by the AgentManager's vision agent
    manager = DataFlowManager()
    await manager.initialize()

    # For now, return success - vision processing is part of agent pipeline
    return {"status": "success", "operation": "groq_vision"}

async def _run_grok_social():
    """Execute Grok social context analysis"""
    # This would be handled by the AgentManager's social agent
    manager = DataFlowManager()
    await manager.initialize()

    # For now, return success - social processing is part of agent pipeline
    return {"status": "success", "operation": "grok_social"}

async def _run_arbitrator():
    """Execute final arbitration"""
    manager = DataFlowManager()
    await manager.initialize()

    picks = await manager.run_agent_pipeline_on_shortlist()

    return {
        "status": "success",
        "operation": "arbitrator",
        "picks_generated": len(picks)
    }

async def _run_publish_to_firebase():
    """Execute Firebase publishing"""
    # Get today's final picks
    from ..core.database import get_database

    db_pool = await get_database()

    query = """
    SELECT symbol, direction, target_low, target_high, arbitrator_confidence, arbitrator_reasoning
    FROM pick_candidates
    WHERE analysis_date = CURRENT_DATE
    AND selected_as_pick = TRUE
    ORDER BY arbitrator_confidence DESC
    """

    async with db_pool.acquire() as conn:
        rows = await conn.fetch(query)

    picks = [dict(row) for row in rows]

    if picks:
        async with FirebaseService() as firebase:
            await firebase.publish_picks(picks)

    return {
        "status": "success",
        "operation": "firebase_publish",
        "picks_published": len(picks)
    }

async def _run_brain_cycle():
    """Execute brain cycle (learning system)"""
    # This would involve the BrainAgent and LearnerAgent
    # For now, return success - learning system to be implemented
    return {"status": "success", "operation": "brain_cycle"}

async def _run_emergency_check():
    """Execute emergency retrain check"""
    kill_switch = KillSwitchService()

    # Check if emergency conditions exist
    is_active = await kill_switch.is_active()

    return {
        "status": "success",
        "operation": "emergency_check",
        "kill_switch_active": is_active
    }

async def _run_bootstrap():
    """Execute one-time database bootstrap"""
    manager = DataFlowManager()
    await manager.initialize()

    # Bootstrap with 90 days of historical data
    result = await manager.bootstrap_historical_data(days_back=90)

    return result

# ==================== MISSING CELERY TASKS FROM TIERED_SCHEDULE ====================

@celery_app.task(name='tasks.fmp_delta_update')
def fmp_delta_update():
    """
    3:00 AM ET â€“ FMP Premium daily delta (1-day bars only)
    Updates ACTIVE tier with latest price data
    """
    # Check if pipeline is enabled
    from ..api.v1.admin import PIPELINE_ENABLED
    if not PIPELINE_ENABLED:
        logger.info("â¸ï¸ Pipeline disabled - skipping FMP delta update")
        return {"status": "skipped", "reason": "pipeline_disabled"}

    logger.info("ðŸ“Š Starting FMP daily delta update")

    try:
        result = asyncio.run(_run_fmp_delta_update())
        logger.info(f"âœ… FMP delta update completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ FMP delta update failed: {e}")
        raise

@celery_app.task(name='tasks.build_active_tickers')
def build_active_tickers():
    """
    3:05 AM ET â€“ Logic filter â†’ ACTIVE (~1,700)
    Apply filtering criteria to classify ALL â†’ ACTIVE
    """
    logger.info("ðŸ” Starting ACTIVE ticker classification")

    try:
        result = asyncio.run(_run_build_active_tickers())
        logger.info(f"âœ… ACTIVE ticker classification completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ ACTIVE ticker classification failed: {e}")
        raise

@celery_app.task(name='tasks.run_finma_prescreen')
def run_finma_prescreen():
    """
    3:10 AM ET â€“ RunPod serverless: FinMA-7b â†’ exactly 75 SHORT_LIST
    """
    logger.info("ðŸ¤– Starting FinMA-7b prescreen")

    try:
        result = asyncio.run(_run_finma_prescreen())
        logger.info(f"âœ… FinMA prescreen completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ FinMA prescreen failed: {e}")
        raise

@celery_app.task(name='tasks.generate_charts')
def generate_charts():
    """
    3:15 AM ET â€“ Generate 75 charts (Matplotlib)
    """
    logger.info("ðŸ“ˆ Starting chart generation")

    try:
        result = asyncio.run(_run_generate_charts())
        logger.info(f"âœ… Chart generation completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Chart generation failed: {e}")
        raise

@celery_app.task(name='tasks.run_groq_vision')
def run_groq_vision():
    """
    3:16 AM ET â€“ Groq Vision â†’ 6 boolean flags
    """
    logger.info("ðŸ‘ï¸ Starting Groq vision analysis")

    try:
        result = asyncio.run(_run_groq_vision())
        logger.info(f"âœ… Groq vision completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Groq vision failed: {e}")
        raise

@celery_app.task(name='tasks.run_grok_social')
def run_grok_social():
    """
    3:17 AM ET â€“ Grok API â†’ social score + news + Polymarket
    """
    logger.info("ðŸŒ Starting Grok social context analysis")

    try:
        result = asyncio.run(_run_grok_social())
        logger.info(f"âœ… Grok social analysis completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Grok social analysis failed: {e}")
        raise

@celery_app.task(name='tasks.run_arbitrator')
def run_arbitrator():
    """
    3:20 AM ET â€“ Rotating Arbitrator â†’ 3â€“6 final picks
    """
    logger.info("âš–ï¸ Starting final arbitration")

    try:
        result = asyncio.run(_run_arbitrator())
        logger.info(f"âœ… Final arbitration completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Final arbitration failed: {e}")
        raise

@celery_app.task(name='tasks.publish_to_firebase')
def publish_to_firebase():
    """
    3:25 AM ET â€“ Push picks to Firebase
    """
    logger.info("ðŸ”¥ Starting Firebase publish")

    try:
        result = asyncio.run(_run_publish_to_firebase())
        logger.info(f"âœ… Firebase publish completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Firebase publish failed: {e}")
        raise

@celery_app.task(name='tasks.run_brain_cycle')
def run_brain_cycle():
    """
    4:01 AM ET â€“ BrainAgent + LearnerAgent â†’ hot-reload 3 files
    """
    logger.info("ðŸ§  Starting brain cycle")

    try:
        result = asyncio.run(_run_brain_cycle())
        logger.info(f"âœ… Brain cycle completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Brain cycle failed: {e}")
        raise

@celery_app.task(name='tasks.check_emergency_retrain')
def check_emergency_retrain():
    """
    Emergency retrain check (every 30 min during market hours)
    """
    logger.info("ðŸš¨ Checking emergency retrain conditions")

    try:
        result = asyncio.run(_run_emergency_check())
        logger.info(f"âœ… Emergency check completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Emergency check failed: {e}")
        raise

# ==================== ONE-TIME BOOTSTRAP TASK ====================

@celery_app.task(name='tasks.run_bootstrap')
def run_bootstrap():
    """
    One-time bootstrap task - Run manually to prime database
    """
    logger.info("ðŸš€ Starting database bootstrap")

    try:
        result = asyncio.run(_run_bootstrap())
        logger.info(f"âœ… Bootstrap completed: {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ Bootstrap failed: {e}")
        raise
