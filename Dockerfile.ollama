# BullsBears AI Inference Server - Official Ollama with GPU Support
FROM ollama/ollama:latest

# Create models directory
RUN mkdir -p /app/models
WORKDIR /app

# Set environment variables
ENV OLLAMA_HOST=0.0.0.0:11434
ENV OLLAMA_MODELS=/app/models

# Expose port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:11434/api/tags || exit 1

# Start Ollama server
CMD ["ollama", "serve"]
